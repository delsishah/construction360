# -*- coding: utf-8 -*-
"""Optimized Heirarchy model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pUZTz1kMG_heOjvwJ25XxfmWA5c5ZSyM
"""

!pip install -q roboflow
from roboflow import Roboflow

rf = Roboflow(api_key="BBD2KSgO7DkV3LT4HKiE")
workspace = rf.workspace()

# Replace these IDs with your actual project IDs
excavating = workspace.project("my-first-project-0hp9o").version(2).download("folder") #rename to My-First-Project-1

!pip install -q roboflow
from roboflow import Roboflow

rf = Roboflow(api_key="P8UG8mKYY92QP5Z517t4")
workspace = rf.workspace()

# Replace these IDs with your actual project IDs
bricklaying = workspace.project("brick-by-brick").version(3).download("folder")
concreting = workspace.project("cementing").version(1).download("folder")

!pip install ultralytics roboflow

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder
from sklearn.model_selection import train_test_split

# Main categories (mutually exclusive)
MAIN_CATEGORIES = ['brick-masonry', 'concreting', 'excavation']

# Sublabels per category (excluding Complete/Incomplete which are shared)
SUBLABELS = {
    'brick-masonry': ['brick-laying', 'capping'],
    'concreting': ['compaction', 'pouring', 'smoothing'],
    'excavation': ['site-clearing', 'digging']
}

# Shared labels
SHARED_LABELS = ['complete', 'incomplete']

def clean_labels(folder_name):
    # Lowercase and split
    labels = folder_name.lower().split()

    # Identify main category (only one)
    main_cat = None
    for mc in MAIN_CATEGORIES:
        if mc in labels:
            main_cat = mc
            break
    if main_cat is None:
        return None, None, None  # invalid folder

    # Separate sublabels (excluding main_cat and shared labels)
    sublabels = [l for l in labels if l != main_cat and l not in SHARED_LABELS]

    # Shared labels: pick complete or incomplete if present (default to incomplete)
    shared = [l for l in labels if l in SHARED_LABELS]
    shared_label = shared[0] if shared else 'incomplete'

    return main_cat, shared_label, sublabels

base_dirs = [
    "/content/My-First-Project-1/train",
    "/content/Cementing-1/train",
    "/content/Brick-by-Brick-3/train"
]

image_paths = []
main_categories = []
shared_labels = []
sublabels_all = []

for base_dir in base_dirs:
    for folder in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, folder)
        if not os.path.isdir(folder_path):
            continue

        main_cat, shared_label, sublabels = clean_labels(folder)
        if main_cat is None:
            print(f"Skipping invalid folder: {folder}")
            continue

        # List all images in this folder
        imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(('.png','.jpg','.jpeg'))]

        image_paths.extend(imgs)
        main_categories.extend([main_cat]*len(imgs))
        shared_labels.extend([shared_label]*len(imgs))
        sublabels_all.extend([sublabels]*len(imgs))

print(f"Loaded {len(image_paths)} images")

import shutil

# Replace with your actual dataset folder name
shutil.rmtree('/content/My-First-Project-2', ignore_errors=True)

# Encode main category (single label)
le_main = LabelEncoder()
main_cat_encoded = le_main.fit_transform(main_categories)  # e.g. brick-masonry -> 0, concreting -> 1, excavation -> 2

# Encode shared labels (complete/incomplete) - binary
le_shared = LabelEncoder()
shared_encoded = le_shared.fit_transform(shared_labels)  # complete->1, incomplete->0

# Create MultiLabelBinarizers for each subcategory
mlb_sub = {}
for cat in MAIN_CATEGORIES:
    mlb_sub[cat] = MultiLabelBinarizer(classes=SUBLABELS[cat])
    mlb_sub[cat].fit([SUBLABELS[cat]])

# Prepare sublabels encoded arrays with zeros for all, will fill accordingly
sublabels_encoded = {
    cat: np.zeros((len(image_paths), len(SUBLABELS[cat]))) for cat in MAIN_CATEGORIES
}

# Fill the appropriate entries in the sublabels_encoded arrays
for i, (main_cat, sublab) in enumerate(zip(main_categories, sublabels_all)):
    # Encode sublabels for this sample according to its main_cat
    # This gives a binary vector for the current sample's sublabels
    encoded_vector = mlb_sub[main_cat].transform([sublab])[0]

    # Place the encoded vector at the correct index 'i' in the array for the main_cat
    sublabels_encoded[main_cat][i] = encoded_vector

# Now sublabels_encoded[cat] will have shape (num_images, num_sublabels_in_cat)

IMG_SIZE = 224
BATCH_SIZE = 32

def load_and_preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])
    img = img / 255.0
    return img

# Split indices for train/val (same for all models)
train_idx, val_idx = train_test_split(np.arange(len(image_paths)), test_size=0.2, stratify=main_cat_encoded, random_state=42)

def create_dataset(indices):
    paths = [image_paths[i] for i in indices]
    main_y = main_cat_encoded[indices]
    shared_y = shared_encoded[indices]
    sub_y = {cat: sublabels_encoded[cat][indices] for cat in MAIN_CATEGORIES}
    return paths, main_y, shared_y, sub_y

train_paths, train_main_y, train_shared_y, train_sub_y = create_dataset(train_idx)
val_paths, val_main_y, val_shared_y, val_sub_y = create_dataset(val_idx)

# Dataset generator function for main model (main category + shared label)
def main_dataset_generator(paths, main_labels, shared_labels):
    def gen():
        for p, m, s in zip(paths, main_labels, shared_labels):
            yield p, (m, s)
    return gen

def preprocess_main_sample(path, labels):
    img = load_and_preprocess_image(path)
    # main label is categorical int -> one-hot
    main_cat_onehot = tf.one_hot(labels[0], len(MAIN_CATEGORIES))
    # shared label is binary scalar
    shared_label = tf.cast(labels[1], tf.float32)
    return img, (main_cat_onehot, shared_label)

train_ds_main = tf.data.Dataset.from_generator(main_dataset_generator(train_paths, train_main_y, train_shared_y),
                                               output_signature=(
                                                   tf.TensorSpec(shape=(), dtype=tf.string),
                                                   (tf.TensorSpec(shape=(), dtype=tf.int32),
                                                    tf.TensorSpec(shape=(), dtype=tf.int32))
                                               ))
train_ds_main = train_ds_main.map(preprocess_main_sample).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

val_ds_main = tf.data.Dataset.from_generator(main_dataset_generator(val_paths, val_main_y, val_shared_y),
                                             output_signature=(
                                                 tf.TensorSpec(shape=(), dtype=tf.string),
                                                 (tf.TensorSpec(shape=(), dtype=tf.int32),
                                                  tf.TensorSpec(shape=(), dtype=tf.int32))
                                             ))
val_ds_main = val_ds_main.map(preprocess_main_sample).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

def build_main_model():
    inp = Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    base_model = EfficientNetB0(include_top=False, input_tensor=inp, weights='imagenet')
    x = GlobalAveragePooling2D()(base_model.output)
    # main category output (softmax)
    main_out = Dense(len(MAIN_CATEGORIES), activation='softmax', name='main_cat')(x)
    # shared output (sigmoid)
    shared_out = Dense(1, activation='sigmoid', name='shared')(x)

    model = Model(inputs=inp, outputs=[main_out, shared_out])
    model.compile(optimizer=Adam(1e-4),
                  loss={'main_cat': 'categorical_crossentropy', 'shared': 'binary_crossentropy'},
                  metrics={'main_cat': 'accuracy', 'shared': 'accuracy'})
    return model

main_model = build_main_model()
main_model.summary()

def build_sub_model(num_labels):
    inp = Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    base_model = EfficientNetB0(include_top=False, input_tensor=inp, weights='imagenet')
    x = GlobalAveragePooling2D()(base_model.output)
    out = Dense(num_labels, activation='sigmoid')(x)
    model = Model(inputs=inp, outputs=out)
    model.compile(optimizer=Adam(1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

sub_models = {}
for cat in MAIN_CATEGORIES:
    sub_models[cat] = build_sub_model(len(SUBLABELS[cat]))
    print(f"{cat} model built.")

def filter_by_category(paths, sub_labels, main_labels, cat):
    # Only select samples belonging to main category cat
    indices = [i for i, m in enumerate(main_labels) if le_main.inverse_transform([m])[0] == cat]
    filtered_paths = [paths[i] for i in indices]
    filtered_sub = sub_labels[indices]
    return filtered_paths, filtered_sub

train_sub_datasets = {}
val_sub_datasets = {}

for cat in MAIN_CATEGORIES:
    tr_paths, tr_sub = filter_by_category(train_paths, train_sub_y[cat], train_main_y, cat)
    val_paths_cat, val_sub = filter_by_category(val_paths, val_sub_y[cat], val_main_y, cat)

    def sub_dataset_generator(paths, labels):
        def gen():
            for p, l in zip(paths, labels):
                yield p, l
        return gen

    def preprocess_sub_sample(path, label):
        img = load_and_preprocess_image(path)
        label = tf.cast(label, tf.float32)
        return img, label

    train_ds = tf.data.Dataset.from_generator(sub_dataset_generator(tr_paths, tr_sub),
                                              output_signature=(
                                                  tf.TensorSpec(shape=(), dtype=tf.string),
                                                  tf.TensorSpec(shape=(len(SUBLABELS[cat]),), dtype=tf.int64)
                                              ))
    train_ds = train_ds.map(preprocess_sub_sample).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    val_ds = tf.data.Dataset.from_generator(sub_dataset_generator(val_paths_cat, val_sub),
                                            output_signature=(
                                                tf.TensorSpec(shape=(), dtype=tf.string),
                                                tf.TensorSpec(shape=(len(SUBLABELS[cat]),), dtype=tf.int64)
                                            ))
    val_ds = val_ds.map(preprocess_sub_sample).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    train_sub_datasets[cat] = train_ds
    val_sub_datasets[cat] = val_ds

# Train Stage 1 model
main_model.fit(train_ds_main,
               validation_data=val_ds_main,
               epochs=3)

# Train Stage 2 submodels
for cat in MAIN_CATEGORIES:
    print(f"Training {cat} sub-model...")
    sub_models[cat].fit(train_sub_datasets[cat],
                        validation_data=val_sub_datasets[cat],
                        epochs=5)

main_model.save("main_model.keras")
sub_models['brick-masonry'].save("submodel_brick-masonry.keras")
sub_models['concreting'].save("submodel_concreting.keras")
sub_models['excavation'].save("submodel_excavation.keras")

from tensorflow.keras.models import load_model

# Load main and submodels
main_model = load_model("main_model.keras")
brick_masonry_model = load_model("submodel_brick-masonry.keras")
concreting_model = load_model("submodel_concreting.keras")
excavation_model = load_model("submodel_excavation.keras")

!ls